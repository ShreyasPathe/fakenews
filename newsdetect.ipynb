{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
      "1  Trump drops Steve Bannon from National Securit...   \n",
      "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
      "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
      "4  Donald Trump heads for Scotland to reopen a go...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
      "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
      "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
      "3  On Monday, Donald Trump once again embarrassed...          News   \n",
      "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
      "\n",
      "                  date  label  \n",
      "0    February 13, 2017      1  \n",
      "1       April 5, 2017       0  \n",
      "2  September 27, 2017       0  \n",
      "3         May 22, 2017      1  \n",
      "4       June 24, 2016       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "df_fake = pd.read_csv(r\"E:\\code\\project\\fakenews\\Fake.csv\")\n",
    "df_true = pd.read_csv(r\"E:\\code\\project\\fakenews\\True.csv\")\n",
    "\n",
    "# Add labels (1 = Fake, 0 = Real)\n",
    "df_fake[\"label\"] = 1\n",
    "df_true[\"label\"] = 0\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([df_fake, df_true], axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display dataset structure\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"clean_text\"] = (df[\"title\"] + \" \" + df[\"text\"]).apply(preprocess_text)\n",
    "\n",
    "# Split dataset (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"clean_text\"], df[\"label\"], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes Model Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert text into numerical vectors\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Naïve Bayes Model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = nb_model.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Naïve Bayes Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(nb_model, \"naive_bayes_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text into sequences\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=300)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pathe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.9255 - loss: 0.1825 - val_accuracy: 0.9918 - val_loss: 0.0251\n",
      "Epoch 2/5\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9975 - loss: 0.0098 - val_accuracy: 0.9931 - val_loss: 0.0213\n",
      "Epoch 3/5\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 36ms/step - accuracy: 0.9996 - loss: 0.0025 - val_accuracy: 0.9947 - val_loss: 0.0203\n",
      "Epoch 4/5\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 3.1483e-04 - val_accuracy: 0.9949 - val_loss: 0.0214\n",
      "Epoch 5/5\n",
      "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9998 - loss: 9.1787e-04 - val_accuracy: 0.9942 - val_loss: 0.0263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tokenizer.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=50, input_length=300),\n",
    "    Conv1D(filters=128, kernel_size=5, activation=\"relu\"),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")  # Output: 0 (Real) or 1 (Fake)\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train CNN model\n",
    "cnn_model.fit(X_train_seq, y_train, epochs=5, batch_size=32, validation_data=(X_test_seq, y_test))\n",
    "\n",
    "# Save CNN Model\n",
    "cnn_model.save(\"cnn_model.h5\")\n",
    "joblib.dump(tokenizer, \"tokenizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming 'X_train' contains your training news articles (processed text)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Set appropriate max features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)  # Fit on training data\n",
    "\n",
    "# **Save the vectorizer**\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "print(\"TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
